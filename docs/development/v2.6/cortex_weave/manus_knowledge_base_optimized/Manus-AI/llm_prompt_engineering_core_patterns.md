name: LLM Prompt Engineering - Core Patterns
use_when: When designing or refining prompts for Large Language Models (LLMs) to ensure clarity, efficiency, and desired output.
content: |
  Effective LLM prompt engineering involves:
  1.  **Task-Specific Prompting**: Clear, concise instructions; imperative language; explicit output formats; examples; defined constraints.
  2.  **Context Management**: Provide relevant, concise context (general to specific); update dynamically; use clear headers.
  3.  **Role-Based Prompting**: Define LLM and user roles; specify relationships; maintain consistency.
  4.  **Few-Shot Learning**: Provide 2-3 high-quality, consistent examples covering edge cases.
  5.  **Chain-of-Thought**: Instruct "think step by step"; break down reasoning; demonstrate steps in examples.
  6.  **Retrieval-Augmented Generation (RAG)**: Integrate vector search for relevant knowledge; use effective chunking and re-ranking; cite sources.
  7.  **Output Structuring**: Define explicit formats (JSON, YAML); use examples; implement schema validation; use delimiters.
  8.  **Error Handling**: Define handling for ambiguous inputs; implement graceful degradation; specify clarification points.
  9.  **Prompt Chaining**: Break complex tasks into sub-prompts; use output as input; preserve context; validate intermediate steps.
  10. **Evaluation & Refinement**: Define criteria; use automated/human evaluation; A/B test; track metrics; version prompts.
  Apply these patterns consistently for high-quality AI interactions.

